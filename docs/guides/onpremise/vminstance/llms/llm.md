---
sidebar_position: 1
---

# éƒ¨ç½²åŸºäº ollama çš„å¤§æ¨¡å‹æœåŠ¡
æœ¬èŠ‚ä»‹ç»å¦‚ä½•åœ¨åœ¨å®¿ä¸»æœºä¸Šé…ç½® nvidia contaienr runtimeï¼Œå¹¶ä½¿ç”¨ gpu åˆ›å»ºä¸€ä¸ªæœ‰ ollama æœåŠ¡çš„å®¹å™¨ä¸»æœº

## å®‰è£…æ˜¾å¡é©±åŠ¨
è¯·è‡ªè¡Œåœ¨[ nvidia å®˜æ–¹](https://www.nvidia.cn/drivers/lookup/)ä¸Šé€‰æ‹©åˆé€‚çš„é—­æºé©±åŠ¨å®‰è£…å¹¶é‡å¯ç³»ç»Ÿ

## é…ç½® nvidia container runtime
```shell
#  nvidia contaienr toolkit,  https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
$ curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
  sudoÂ  Â 
tee /etc/yum.repos.d/nvidia-container-toolkit.repo
Â 
Â 
Â  Â 
$ sudo yum-config-manager --enable nvidia-container-toolkit-experimental
Â  Â 
$ sudo yum install -y nvidia-container-toolkit
Â 
Â 
#  containerd runtime,  --config  containerd 
$ nvidia-ctk runtime configure --runtime=containerd --config /etc/yunion/containerd/config.toml
```
æ‰§è¡Œä¸Šè¯‰æ­¥éª¤åéœ€è¦æ‰‹åŠ¨ä¿®æ”¹ containerd runtime åˆ‡æ¢æˆ nvidia
```shell
#  runc  nvidia
$ viÂ /etc/yunion/containerd/config.toml
default_runtime_name = "runc"
---
default_runtime_name = "nvidia"
Â 
Â 
#  yunion containerd
$ systemctl restart yunion-containerd.service
```

<!--
### ä½¿ç”¨éå®˜æ–¹çš„ GGUF æ–‡ä»¶åˆ›å»º
é¦–å…ˆåœ¨[ Hhugging Face ](https://huggingface.co/)æˆ–è€…[é­”æ­ç¤¾åŒº](https://modelscope.cn/)æŒ‘é€‰ä¸€ä¸ªåˆé€‚çš„æ¨¡å‹ï¼Œç„¶åä½¿ç”¨ gguf æ–‡ä»¶é“¾æ¥è¿›è¡Œåˆ›å»º
```shell
climc llm-create --allow-delete\
  --disk medium=rotate,size=128m,format=raw\
  --net cnet18 -p 'index=0,port=11434,host_port=20435,protocol=tcp'\
  test-llm-create-$(date +'%Y%m%d%H%M%S') 4096\
  docker.io/ollama/ollama:latest qwen2:7b\
  --gguf "file=https://modelscope.cn/models/qwen/Qwen2-7B-Instruct-GGUF/resolve/master/qwen2-7b-instruct-q2_k.gguf,source=web"
```
è‹¥å®˜æ–¹æœªæä¾›ï¼Œå¯å°è¯•è‡ªè¡Œä½¿ç”¨[ llama.cpp ](https://github.com/ggml-org/llama.cpp)å°†å…¶è½¬åŒ–ä¸º gguf æ–‡ä»¶ï¼Œå¹¶ä¸Šä¼ åˆ°å®¿ä¸»æœºä¸Šè¿›è¡Œæ¨¡å‹åˆ›å»º
```shell
climc llm-create --allow-delete\
  --disk medium=rotate,size=128m,format=raw\
  --net cnet18 -p 'index=0,port=11434,host_port=20435,protocol=tcp'\
  test-llm-create-$(date +'%Y%m%d%H%M%S') 4096\
  docker.io/ollama/ollama:latest qwen2:0.5b\
  --gguf "file=/root/qwen2-0_5b.gguf,source=host,parameter=[temperature=0.6|stop=AI assistant:|num_ctx=2048|top_k=100]"
```
å¯¹äº modelfile å‚æ•°çš„å«ä¹‰åŠè®¾ç½®æ–¹å¼ï¼Œå…·ä½“å¯å‚è€ƒ[ ollama çš„å®˜æ–¹æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)ï¼Œå…¶ä¸­çš„ `ADAPTER` å‚æ•°æš‚æœªæ”¯æŒ -->

## åˆ›å»º Ollama é•œåƒ
ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åˆ›å»ºä¸€ä¸ª `Ollama` é•œåƒæ¡ç›®
```sh
climc llm-image-create ollama-latest <YOUR-OLLAMA-IMAGE-NAME> latest
```

## åˆ›å»º llm æ¨¡æ¿
ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åˆ›å»ºä¸€ä¸ª `llm` çš„æ„å»ºæ¨¡æ¿
```sh
climc llm-model-create ollama-qwen3-8b \
  1 1023 1024 ollama-latest ollama qwen3:8b \
  --devices 'GeForce RTX 4060' --port-mappings 'tcp:11434'
```

## ä½¿ç”¨ llm æ„å»ºæ¨¡æ¿å¯åŠ¨ä¸€ä¸ª qwen3:8b çš„å¤§æ¨¡å‹æœåŠ¡
ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åˆ›å»ºï¼š
```sh
climc llm-create qwen3-8b ollama-qwen3-8b
```

## è®¿é—®å¤§æ¨¡å‹æœåŠ¡
å¯ä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•è¿›è¡Œæµ‹è¯•
```shell
curl http://cloudpods-ip:host-port/v1/chat/completions\
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3:8b",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'
```
å¦‚æœéƒ¨ç½²æˆåŠŸä¼šæœ‰ç±»ä¼¼ä¸‹é¢çš„è¾“å‡º
```json
{
  "id":"chatcmpl-991",
  "object":"chat.completion",
  "created":1761122201,
  "model":"qwen3:8b",
  "system_fingerprint":"fp_ollama",
  "choices":[{
    "index":0,
    "message":{
      "role":"assistant",
      "content":"\u003cthink\u003e\nOkay, the user greeted me with \"Hello, how are you?\" I need to respond appropriately. First, I should acknowledge their greeting and express that I'm here to help. Since I'm an AI, I don't have feelings, so I should mention that I don't have emotions but can assist with various tasks. I should keep the tone friendly and open-ended to encourage them to ask questions or share what they need help with. Maybe add an emoji to keep it approachable. Let me check for any errors and make sure the response is clear and welcoming.\n\u003c/think\u003e\n\nHello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need! ğŸ˜Š What can I assist you with today?"
    },
    "finish_reason":"stop"
  }],
  "usage":{
    "prompt_tokens":14,
    "completion_tokens":159,
    "total_tokens":173
  }
}
```